{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfXfovkrW2G6"
   },
   "source": [
    "# Document Filtering\n",
    "\n",
    "Ch 6 from *Programming Collective Intelligence*, based on code from\n",
    "* https://github.com/arthur-e/Programming-Collective-Intelligence/tree/master/chapter6\n",
    "* https://go.oreilly.com/old-dominion-university/library/view/programming-collective-intelligence/9780596529321/\n",
    "\n",
    "**Goal:** Classify email as spam or not spam.\n",
    "\n",
    "**Implemented Example:** Classify a given document as \"bad\" or \"good\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XChxdkXms_XE"
   },
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OHgwbcheq5QS"
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sqlite   # replaces import stmt from book\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoeWBsbiYMrP"
   },
   "source": [
    "`getwords(doc)` - returns a list of unique words found in the given document\n",
    "\n",
    "* breaks up the text into words, by dividing on any character that isn’t a letter\n",
    "* leaves only actual words, converted to lowercase\n",
    "* returns only unique words (so doesn't calculate the count if a word is used multiple times in a document)\n",
    "\n",
    "Note that this reduces the number of features because text is now case insensitive. However, this will completely miss ALL CAPS as potential feature for spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BQw0CH-MrJq7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\JHON G. BOTELLO\\AppData\\Local\\Temp\\ipykernel_29444\\2609612855.py:2: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  splitter=re.compile('\\W+')  # different than book\n"
     ]
    }
   ],
   "source": [
    "def getwords(doc):\n",
    "  splitter=re.compile('\\W+')  # different than book\n",
    "  #print (doc)\n",
    "  # Split the words by non-alpha characters\n",
    "  words=[s.lower() for s in splitter.split(doc) \n",
    "          if len(s)>2 and len(s)<20]\n",
    "  \n",
    "  # Return the unique set of words only\n",
    "  uniq_words = dict([(w,1) for w in words])\n",
    "\n",
    "  return uniq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBB62YFRsfgZ"
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_SEu0QhwDlS"
   },
   "source": [
    "*To use this with the basic classifier (and to change it back later), make the following changes:*\n",
    "* `class naivebayes(classifier)` -> `class naivebayes(basic_classifier)`\n",
    "* `classifier.__init__(self,getfeatures)` -> `basic_classifier.__init__(self,getfeatures)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Modified Naive Bayes Classifier (change basic_classifier if needed)\n",
    "class naivebayes(basic_classifier):   # change for basic_classifier\n",
    "\n",
    "  def __init__(self,getfeatures):   \n",
    "    basic_classifier.__init__(self,getfeatures)  # change for basic_classifier\n",
    "    self.thresholds={}\n",
    "  \n",
    "  def docprob(self,item,cat):\n",
    "    features=self.getfeatures(item)   \n",
    "\n",
    "    # Multiply the probabilities of all the features together\n",
    "    p=1\n",
    "    for f in features: p*=self.weightedprob(f,cat,self.fprob)\n",
    "    return p\n",
    "\n",
    "  def prob(self,item,cat):\n",
    "    catprob=self.catcount(cat)/self.totalcount()\n",
    "    docprob=self.docprob(item,cat)\n",
    "    return docprob*catprob\n",
    "  \n",
    "  def setthreshold(self,cat,t):\n",
    "    self.thresholds[cat]=t\n",
    "    \n",
    "  def getthreshold(self,cat):\n",
    "    if cat not in self.thresholds: return 1.0\n",
    "    return self.thresholds[cat]\n",
    "  \n",
    "  def classify(self,item,default=None):\n",
    "    probs={}\n",
    "    # Find the category with the highest probability\n",
    "    max=0.0\n",
    "    for cat in self.categories():\n",
    "      probs[cat]=self.prob(item,cat)\n",
    "      if probs[cat]>max: \n",
    "        max=probs[cat]\n",
    "        best=cat\n",
    "\n",
    "    # Make sure the probability exceeds threshold*next best\n",
    "    for cat in probs:\n",
    "      if cat==best: continue\n",
    "      if probs[cat]*self.getthreshold(best)>probs[best]: return default\n",
    "    return best\n",
    "\n",
    "# Function to read content of emails\n",
    "def read_emails_from_folder(folder_path):\n",
    "    emails = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                emails.append(content)\n",
    "    return emails\n",
    "\n",
    "classifier = naivebayes(getwords)\n",
    "\n",
    "# Training the Classifier\n",
    "work_folder = \"C:/Users/JHON G. BOTELLO/OneDrive - Old Dominion University/PHD/Courses/Spring 2024/Web Science/Web-Science/HW7-Email Classification/Code/Data/Training/work\"\n",
    "nonwork_folder = \"C:/Users/JHON G. BOTELLO/OneDrive - Old Dominion University/PHD/Courses/Spring 2024/Web Science/Web-Science/HW7-Email Classification/Code/Data/Training/nonwork\"\n",
    "\n",
    "work_emails = read_emails_from_folder(work_folder)\n",
    "nonwork_emails = read_emails_from_folder(nonwork_folder)\n",
    "\n",
    "for email in work_emails:\n",
    "    classifier.train(email, \"work\")\n",
    "\n",
    "for email in nonwork_emails:\n",
    "    classifier.train(email, \"nonwork\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Results:\n",
      "Actual         Predicted      \n",
      "work           nonwork        \n",
      "work           nonwork        \n",
      "work           work           \n",
      "work           work           \n",
      "work           work           \n",
      "nonwork        nonwork        \n",
      "nonwork        nonwork        \n",
      "nonwork        nonwork        \n",
      "nonwork        nonwork        \n",
      "nonwork        nonwork        \n"
     ]
    }
   ],
   "source": [
    "# Testing the Classifier\n",
    "test_work_folder = \"C:/Users/JHON G. BOTELLO/OneDrive - Old Dominion University/PHD/Courses/Spring 2024/Web Science/Web-Science/HW7-Email Classification/Code/Data/Testing/work\"\n",
    "test_nonwork_folder = \"C:/Users/JHON G. BOTELLO/OneDrive - Old Dominion University/PHD/Courses/Spring 2024/Web Science/Web-Science/HW7-Email Classification/Code/Data/Testing/nonwork\"\n",
    "\n",
    "test_work_emails = read_emails_from_folder(test_work_folder)\n",
    "test_nonwork_emails = read_emails_from_folder(test_nonwork_folder)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Classify work emails\n",
    "for email in test_work_emails:\n",
    "    prediction = classifier.classify(email)\n",
    "    results.append((\"work\", prediction))\n",
    "\n",
    "# Classify nonwork emails\n",
    "for email in test_nonwork_emails:\n",
    "    prediction = classifier.classify(email)\n",
    "    results.append((\"nonwork\", prediction))\n",
    "\n",
    "# Print the results as a table\n",
    "print(\"\\nTesting Results:\")\n",
    "print(f\"{'Actual':<15}{'Predicted':<15}\")\n",
    "for actual, predicted in results:\n",
    "    print(f\"{actual:<15}{predicted:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassified Emails:\n",
      "Actual: work, Predicted: nonwork\n",
      "Content: 1 new citation to your articles\n",
      "---------- Forwarded message ---------\n",
      "From: Google Scholar Alerts <scholaralerts-noreply@google.com>\n",
      "Date: Sat, Nov 30, 2024 at 4:12 AM\n",
      "Subject: 1 new citation to your articles\n",
      "\n",
      "[HTML] Storyline Extraction of Document-Level Events Using Large Language Models\n",
      "Z Hu, Y Li - Journal of Computer and Communications, 2024\n",
      "This article proposes a document-level prompt learning approach using LLMs to\n",
      "extract the timeline-based storyline. Through verification tests on datasets such as\n",
      "ESCv1. 2 and Timeline17, the results show that the prompt+ one-shot learning\n",
      "proposed in this article works well. Meanwhile, our research findings indicate that\n",
      "although timeline-based storyline extraction has shown promising prospects in the\n",
      "practical applications of LLMs, it is still a complex natural language processing task …\n",
      "•\tCites: ‪Generating stories from archived collections‬  Edit\n",
      "Save\tTwitter\t\n",
      "\n",
      "\n",
      "Weigle, Michele C.\n",
      "​\n",
      "FREW, LESLEY E.;\n",
      "​+2 others\n",
      "​\n",
      "​\n",
      "Might be interesting to check out.\n",
      "\n",
      "Actual: work, Predicted: nonwork\n",
      "Content: Thoughts on this?\n",
      "\n",
      "Padilla, Jose J.\n",
      "​\n",
      "LLINAS MARIMON, BRIAN;\n",
      "​+2 others\n",
      "​\n",
      "​\n",
      "This project will generate original data to study how climate and environmental stressors shape a broad range of individual and group adaptation behaviors and which government-level adaptations reduce the prevalence of socially problematic adaptations, such as household migration or local inter-group violence, which often precede wider societal disruptions. To generate original data on climate adaptation policies and behaviors at the individual, social group, and governmental levels, we will use a series of fine-tuned Large Language Models (LLMs) applied to a unique corpus of 120 million articles published by local news outlets based in more than 60 developing countries from 2012-2024. This will allow us to create a monthly, sub-national (ADM1) dataset on 10 distinct types of environmental adaptations never before measured at such scale. We will also use this method to generate new media-derived data for reporting on sudden onset weather and environmental events as well as slow onset environmental change that we expect to cause adaptation behaviors. We will complement these data with forthcoming data from the International Organization of Migration (IOM), publicly available project-level data on adaptation-focused development aid from the OECD, and high-resolution data on climate and environmental stressors.\n",
      "\n",
      " \n",
      "\n",
      "Thoughts?\n",
      "\n",
      " \n",
      "\n",
      "https://minerva.defense.gov/Research/Funded-Projects/Article/3956358/modeling-climate-induced-societal-adaptation-and-population/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze misclassified emails\n",
    "print(\"\\nMisclassified Emails:\")\n",
    "for (actual, predicted), email in zip(results, test_work_emails + test_nonwork_emails):\n",
    "    if actual != predicted:\n",
    "        print(f\"Actual: {actual}, Predicted: {predicted}\")\n",
    "        print(f\"Content: {email}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyORuunFNMXFH8fBLXsCrog1",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "432-PCI-Ch06.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
